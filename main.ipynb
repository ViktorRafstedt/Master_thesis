{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import fitz  #PyMuPDF\n",
    "\n",
    "def pdf_to_text(file_path):\n",
    "    with fitz.open(file_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf:\n",
    "            text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pdf_directory_vw = #Your local directory to folder of volkswagen reports\n",
    "pdf_directory_psa = #Your local directory to folder of groupePSA reports\n",
    "pdf_directory_vc = #Your local directory to folder of volvo cars reports\n",
    "file_list_vw = os.listdir(pdf_directory_vw)\n",
    "file_list_psa = os.listdir(pdf_directory_psa)\n",
    "file_list_vc = os.listdir(pdf_directory_vc)\n",
    "\n",
    "data_vw = []\n",
    "data_psa = []\n",
    "data_vc = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Below cell only needs to run if there are no CSV files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all Volkswagen files\n",
    "for file_name in file_list_vw:\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_directory_vw, file_name)\n",
    "        year = int(file_name.split('_')[-1].split('.')[0]) \n",
    "        firm = file_name.split('_')[0]\n",
    "\n",
    "        text = pdf_to_text(file_path)\n",
    "        data_vw.append({'firm': firm, 'year': year, 'text': text})\n",
    "        print('File completed')\n",
    "\n",
    "print('Volkswagen Completed')\n",
    "\n",
    "\n",
    "#Convert all GroupPSA files\n",
    "for file_name in file_list_psa:\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_directory_psa, file_name)\n",
    "        year = int(file_name.split('_')[-1].split('.')[0])\n",
    "        firm = file_name.split('_')[0]\n",
    "\n",
    "        text = pdf_to_text(file_path)\n",
    "        data_psa.append({'firm': firm, 'year': year, 'text': text})\n",
    "        print('File completed')\n",
    "\n",
    "print('PSA Completed')\n",
    "\n",
    "\n",
    "#Convert all VolvoCars files\n",
    "for file_name in file_list_vc:\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(pdf_directory_vc, file_name)\n",
    "        year = int(file_name.split('_')[-1].split('.')[0])\n",
    "        firm = file_name.split('_')[0]\n",
    "\n",
    "        text = pdf_to_text(file_path)\n",
    "        data_vc.append({'firm': firm, 'year': year, 'text': text})\n",
    "        print('File completed')\n",
    "\n",
    "print('Volvo Completed')\n",
    "\n",
    "#Make dataframes\n",
    "df_vw = pd.DataFrame(data_vw)\n",
    "df_psa = pd.DataFrame(data_psa)\n",
    "df_vc = pd.DataFrame(data_vc)\n",
    "\n",
    "#Save as CSV file\n",
    "#Create the folder if it doesn't exist\n",
    "if not os.path.exists('CSVFiles'):\n",
    "    os.makedirs('CSVFiles')\n",
    "df_vw.to_csv('CSVFiles/df_vw_2.csv', index=False)\n",
    "df_psa.to_csv('CSVFiles/df_psa_2.csv', index=False)\n",
    "df_vc.to_csv('CSVFiles/df_vc_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert CSif os.path.exists('CSVFiles'):\n",
    "if os.path.exists('CSVFiles'):    \n",
    "    df_vw = pd.read_csv('CSVFiles/df_vw_2.csv')\n",
    "    df_psa = pd.read_csv('CSVFiles/df_psa_2.csv')\n",
    "    df_vc = pd.read_csv('CSVFiles/df_vc_2.csv')\n",
    "else:\n",
    "    print(\"CSVFiles directory does not exist.\")\n",
    "\n",
    "df_vc[\"text\"] = df_vc[\"text\"].astype(str)\n",
    "df_vw[\"text\"] = df_vw[\"text\"].astype(str)\n",
    "df_psa[\"text\"] = df_psa[\"text\"].astype(str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_keywords_1 = ['air', 'biodiversity', 'carbon', 'circularity', 'climate',     \n",
    "                'compliance', 'community', 'diversity', 'emission',     \n",
    "                'energy', 'ethic', 'governance', 'greenhouse', 'hazardous',     \n",
    "                'human', 'inclusion', 'land', 'labor', 'pollution',     \n",
    "                'recycling', 'renewable', 'risk', 'social', 'stakeholder',     \n",
    "                'sustainability', 'supply', 'transparency', 'water',    \n",
    "                'efficiency', 'health', 'safety', 'chain', 'management',     \n",
    "                'alternative', 'autonomous', 'biofuel', 'catalyst', 'charge',     \n",
    "                'clean', 'collaboration', 'conservation', 'consumption', 'demand',     \n",
    "                'depletion', 'development', 'digital', 'disclosure', 'effluent',     \n",
    "                'electrification', 'empathy', 'endangered', 'engagement', 'engineering',     \n",
    "                'environmental', 'ethical', 'extinction', 'fleet',     \n",
    "                'green', 'habitat', 'hybrid', 'impact', 'infrastructure',     \n",
    "                'innovation', 'just', 'local', 'low', 'materiality',     \n",
    "                'mobility', 'natural', 'nonrenewable', 'oil', 'particulate',     \n",
    "                'planet', 'plastic', 'powertrain', 'provenance', 'quality',     \n",
    "                'range', 'recyclable', 'regulation', 'renewable', 'resilience',     \n",
    "                'responsibility', 'reuse', 'smart', 'stewardship', 'sustainable',     \n",
    "                'technology', 'transportation', 'urban', 'waste',     \n",
    "                'wellbeing', 'wildlife', 'emissionless', 'electric', 'bioenergy', \n",
    "                'biomaterial', 'upcycling', 'solar', 'ecodesign', 'offset', \n",
    "                'microplastic', 'e-waste', 'afforestation', 'reforestation', \n",
    "                'decarbonization', 'accident' 'bikeability', 'walkability', 'noise', \n",
    "                'bioeconomy', 'low-carbon', 'retrofit', 'cogeneration',\n",
    "                'degrowth', 'biodegradable', 'zero-waste', 'carpooling', \n",
    "                'ridesharing', 'emobility', 'xeriscaping', 'remanufacturing', \n",
    "                'regenerative', 'telecommuting', 'refurbishment', 'upgradable', \n",
    "                'public', 'adaptation', 'mitigation', 'volatility', \n",
    "                'traceability', 'reforestation', \n",
    "                'microgrid', 'co2-neutral', 'remediation', \n",
    "                'revitalization', 'multimodal', 'intermodal', 'interconnected', \n",
    "                'livability', 'reusability', 'repurposing', \n",
    "                'eco-efficiency', 'air_quality', 'carbon-neutral', 'cogeneration', \n",
    "                'greenbuilding', 'eco-city', 'renewable', 'bioplastic', \n",
    "                'biodegradation', 'bioremediation', 'permaculture', \n",
    "                'urban_farming', 'agroforestry', 'geothermal', 'wind_power', \n",
    "                'hydropower', 'carbon_sequestration', 'carbon_capturing', \n",
    "                'carsharing', 'scootersharing', 'bikesharing', 'eco-driving', \n",
    "                'composting', \"capture\", \"gri\"\n",
    "]\n",
    "\n",
    "esg_keywords = [\"risk\", \"development\", \"emission\", \"environmental\", \n",
    "                \"water\", \"supply\", \"human\", \"climate\", \"sustainability\", \n",
    "                \"safety\", \"management\", \"energy\", \"decarbonization\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "#Set the NLTK data path to your desired directory\n",
    "nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#Download required NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "remove_list = [\"sustainability report\", \"corporate social responsibility\", \"strategic guidelines, commitments and indicators\", \"strategy\\nrisk\\ndirectors\"]\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    #Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    for remove_str in remove_list:\n",
    "        text = text.replace(remove_str, \"\")\n",
    "        \n",
    "    #Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    #Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\")) - set(esg_keywords)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    #Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "#Preprocess text in each dataframe\n",
    "df_vc[\"processed_text\"] = df_vc[\"text\"].apply(preprocess_text)\n",
    "df_vw[\"processed_text\"] = df_vw[\"text\"].apply(preprocess_text)\n",
    "df_psa[\"processed_text\"] = df_psa[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency Analysis of ESG Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filter_esg_words(word_count, esg_keywords):\n",
    "    filtered_word_count = {word: count for word, count in word_count.items() if word in esg_keywords}\n",
    "    return filtered_word_count\n",
    "\n",
    "def aggregate_word_counts(df):\n",
    "    aggregated_counts = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        words = row['processed_text'].split()\n",
    "        aggregated_counts.update(words)\n",
    "    return aggregated_counts\n",
    "\n",
    "def count_words(words):\n",
    "    if not isinstance(words, (list, str)):\n",
    "        return Counter()\n",
    "    if isinstance(words, str):\n",
    "        words = words.split()\n",
    "    word_count = Counter(words)\n",
    "    return word_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to extract ESG keywords from processed text\n",
    "def extract_esg_words(text):\n",
    "    #Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    #Keep only ESG keywords\n",
    "    esg_words = [word for word in words if word in esg_keywords]\n",
    "\n",
    "    return \" \".join(esg_words)\n",
    "\n",
    "#Add \"esg_words\" column to each dataframe\n",
    "df_vw[\"esg_words\"] = df_vw[\"processed_text\"].apply(extract_esg_words)\n",
    "df_psa[\"esg_words\"] = df_psa[\"processed_text\"].apply(extract_esg_words)\n",
    "df_vc[\"esg_words\"] = df_vc[\"processed_text\"].apply(extract_esg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(df):\n",
    "    if df is df_vw:\n",
    "        return \"Volkswagen\"\n",
    "    elif df is df_psa:\n",
    "        return \"GroupePSA\"\n",
    "    elif df is df_vc:\n",
    "        return \"Volvo Cars\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define a function to count the frequency of each word in a text string\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):\n",
    "        return Counter(text.split())\n",
    "    else:\n",
    "        return Counter()\n",
    "\n",
    "#Create a list of dataframes\n",
    "dfs = [df_vw, df_psa, df_vc]\n",
    "\n",
    "#Loop through each dataframe\n",
    "for df in dfs:\n",
    "    #Group the data by year\n",
    "    groups = df.groupby('year')\n",
    "\n",
    "    #Initialize an empty DataFrame to store the word counts\n",
    "    word_counts = pd.DataFrame()\n",
    "\n",
    "    #Loop over the groups and count the frequency of each word for each year\n",
    "    for year, group in groups:\n",
    "        text = ' '.join(group['esg_words'])\n",
    "        counts = count_words(text)\n",
    "        counts_df = pd.DataFrame({'year': year, 'word': list(counts.keys()), 'count': list(counts.values())})\n",
    "        word_counts = pd.concat([word_counts, counts_df], ignore_index=True)\n",
    "\n",
    "    #Pivot the word_counts DataFrame to create a table with words as rows and years as columns\n",
    "    word_counts_pivot = word_counts.pivot(index='word', columns='year', values='count')\n",
    "    #Get the top 20 most common words\n",
    "    top_words = word_counts.groupby('word')['count'].sum().nlargest(20).index\n",
    "    #Filter the pivot table to include only the top 10 words\n",
    "    word_counts_top = word_counts_pivot.loc[top_words]\n",
    "\n",
    "    #Create a heatmap of the word counts\n",
    "    plt.imshow(word_counts_top, cmap='Greens', aspect='auto')\n",
    "    plt.xticks(range(len(word_counts_top.columns)), word_counts_top.columns, rotation=45)\n",
    "    plt.yticks(range(len(word_counts_top.index)), word_counts_top.index)\n",
    "\n",
    "    #Annotate the heatmap with the exact counts of each word in each cell\n",
    "    for i in range(len(word_counts_top.index)):\n",
    "        for j in range(len(word_counts_top.columns)):\n",
    "            count = word_counts_top.iloc[i, j]\n",
    "            plt.text(j, i, str(int(count)), ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.title('Word counts by year: ' + get_company_name(df))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "\n",
    "#Define a function to count the frequency of each word in a text string\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        #Merge the counts of \"sustainable\" and \"sustainability\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"sustainable\":\n",
    "                words[i] = \"sustainability\"\n",
    "        return Counter(words)\n",
    "    else:\n",
    "        return Counter()\n",
    "\n",
    "#Create a list of dataframes\n",
    "dfs = [df_vw, df_psa, df_vc]\n",
    "\n",
    "#Loop through each dataframe\n",
    "for df in dfs:\n",
    "    #Group the data by year\n",
    "    groups = df.groupby('year')\n",
    "\n",
    "    #Initialize an empty DataFrame to store the word counts\n",
    "    word_counts = pd.DataFrame()\n",
    "\n",
    "    #Loop over the groups and count the frequency of each word for each year\n",
    "    for year, group in groups:\n",
    "        text = ' '.join(group['esg_words'])\n",
    "        counts = count_words(text)\n",
    "        counts_df = pd.DataFrame({'year': year, 'word': list(counts.keys()), 'count': list(counts.values())})\n",
    "        word_counts = pd.concat([word_counts, counts_df], ignore_index=True)\n",
    "\n",
    "    #Pivot the word_counts DataFrame to create a table with words as rows and years as columns\n",
    "    word_counts_pivot = word_counts.pivot(index='word', columns='year', values='count')\n",
    "    \n",
    "    #Get the top 50 most common words\n",
    "    top_words = word_counts.groupby('word')['count'].sum().nlargest(100).index\n",
    "    #Filter the pivot table to include only the top 50 words\n",
    "    word_counts_top = word_counts_pivot.loc[top_words]\n",
    "\n",
    "    #Replace all NaN values with 0\n",
    "    word_counts_top = word_counts_top.fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    #Create a heatmap of the word counts\n",
    "    plt.imshow(word_counts_top, cmap='Greens', aspect='auto')\n",
    "    plt.xticks(range(len(word_counts_top.columns)), word_counts_top.columns, rotation=45)\n",
    "    plt.yticks(range(len(word_counts_top.index)), word_counts_top.index)\n",
    "\n",
    "    #Annotate the heatmap with the exact counts of each word in each cell\n",
    "    for i in range(len(word_counts_top.index)):\n",
    "        for j in range(len(word_counts_top.columns)):\n",
    "            count = word_counts_top.iloc[i, j]\n",
    "            plt.text(j, i, str(int(count)), ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.title('Word counts by year: ' + get_company_name(df))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def process_dataframe(df):\n",
    "    df_new = df.copy()\n",
    "    df_new['amount_words'] = df_new['text'].apply(count_words)\n",
    "    df_new = df_new[['firm', 'year', 'amount_words']]\n",
    "    df_new = df_new.sort_values(by='year', ascending=True)\n",
    "    return df_new\n",
    "\n",
    "df_vw_text = process_dataframe(df_vw)\n",
    "df_vc_text = process_dataframe(df_vc)\n",
    "df_psa_text = process_dataframe(df_psa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'TeX Gyre Termes'\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "def plot_dataframe(df, title):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    df_filtered = df[df['amount_words'] >= 100]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_filtered['year'], df_filtered['amount_words'], marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "    \n",
    "    plt.xlabel('Year', fontsize=14, labelpad=15)\n",
    "    plt.ylabel('Amount of Words', fontsize=14, labelpad=15)\n",
    "    plt.title(title, fontsize=16, pad=20)\n",
    "    \n",
    "    plt.xticks(df_filtered['year'].unique(), fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_dataframe(df_vw_text, 'Volkswagen Word Count by Year')\n",
    "plot_dataframe(df_vc_text, 'Volvo Cars Word Count by Year')\n",
    "plot_dataframe(df_psa_text, 'PSA Group Word Count by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'TeX Gyre Termes'\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "\n",
    "def plot_dataframe(df1, df2, df3, labels, colors):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    df1_filtered = df1[df1['amount_words'] >= 100]\n",
    "    df2_filtered = df2[df2['amount_words'] >= 100]\n",
    "    df3_filtered = df3[df3['amount_words'] >= 100]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df1_filtered['year'], df1_filtered['amount_words'], marker='o', linewidth=2, markersize=8, label=labels[0], color=colors[0])\n",
    "    plt.plot(df2_filtered['year'], df2_filtered['amount_words'], marker='o', linewidth=2, markersize=8, label=labels[1], color=colors[1])\n",
    "    plt.plot(df3_filtered['year'], df3_filtered['amount_words'], marker='o', linewidth=2, markersize=8, label=labels[2], color=colors[2])\n",
    "    \n",
    "    plt.xlabel('Year', fontsize=14, labelpad=15)\n",
    "    plt.ylabel('Amount of Words', fontsize=14, labelpad=15)\n",
    "    plt.title('Word Count by Year', fontsize=16, pad=20)\n",
    "    \n",
    "    years = set(df1_filtered['year']).union(df2_filtered['year']).union(df3_filtered['year'])\n",
    "    plt.xticks(sorted(list(years)), fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_dataframe(df_vw_text, df_vc_text, df_psa_text, ['Volkswagen', 'Volvo Cars', 'PSA Group'], ['darkgreen', 'darkblue', 'darkorange'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#Define a function to count the frequency of each word in a text string\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.split()\n",
    "        #Merge the counts of \"sustainable\" and \"sustainability\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"sustainable\":\n",
    "                words[i] = \"sustainability\"\n",
    "        return Counter(words)\n",
    "    else:\n",
    "        return Counter()\n",
    "\n",
    "# Define a dictionary to map words to categories\n",
    "categories = {\n",
    "    'Social': ['diversity', 'inclusion', 'human', 'social', 'community', 'labour'],\n",
    "    'Ecological': ['environmental', 'carbon', 'climate', 'emission'],\n",
    "    'Technologies': ['electric', 'hybrid', 'electrification'],\n",
    "    'Emerging technologies/trends': ['circular', 'decarbonization', 'innovation'],\n",
    "    'Risk': ['risk'],\n",
    "    'Regulatory': ['compliance', 'regulation', 'disclosure'],\n",
    "    'Governance': ['governance', 'management', 'stakeholder']\n",
    "}\n",
    "\n",
    "# Create a list of dataframes\n",
    "dfs = [df_vw, df_psa, df_vc]\n",
    "\n",
    "# Loop through each dataframe\n",
    "for df in dfs:\n",
    "    # Group the data by year\n",
    "    groups = df.groupby('year')\n",
    "\n",
    "    # Initialize an empty DataFrame to store the word counts\n",
    "    word_counts = pd.DataFrame()\n",
    "\n",
    "    # Loop over the groups and count the frequency of each word for each year\n",
    "    for year, group in groups:\n",
    "        # Concatenate all preprocessed texts for the year\n",
    "        text = ' '.join(group['esg_words'])\n",
    "        # Count the frequency of each word\n",
    "        counts = count_words(text)\n",
    "\n",
    "        # Add category counts\n",
    "        for category, words in categories.items():\n",
    "            category_count = sum(counts[word] for word in words if word in counts)\n",
    "            counts[category] = category_count\n",
    "\n",
    "        # Store the counts in a DataFrame, with columns for the year and word\n",
    "        counts_df = pd.DataFrame({'year': year, 'word': list(counts.keys()), 'count': list(counts.values())})\n",
    "        # Append the counts to the word_counts DataFrame\n",
    "        word_counts = pd.concat([word_counts, counts_df], ignore_index=True)\n",
    "\n",
    "    # Pivot the word_counts DataFrame to create a table with words as rows and years as columns\n",
    "    word_counts_pivot = word_counts.pivot(index='word', columns='year', values='count')\n",
    "    \n",
    "    # Get the top 50 most common words\n",
    "    top_words = word_counts.groupby('word')['count'].sum().nlargest(100).index\n",
    "    \n",
    "    # Filter the pivot table to include only the top 50 words\n",
    "    word_counts_top = word_counts_pivot.loc[top_words]\n",
    "\n",
    "    # Replace all NaN values with 0\n",
    "    word_counts_top = word_counts_top.fillna(0)\n",
    "\n",
    "    # Remove columns (years) where all values are 0 (no report exists)\n",
    "    word_counts_top = word_counts_top.loc[:, (word_counts_top != 0).any(axis=0)]\n",
    "\n",
    "    # First, create a new list to store the words sorted by categories\n",
    "    sorted_words = []\n",
    "\n",
    "    # Loop through the categories and add the category name followed by its words\n",
    "    for category, words in categories.items():\n",
    "        sorted_words.append(category)\n",
    "        for word in words:\n",
    "            if word in word_counts_top.index:\n",
    "                sorted_words.append(word)\n",
    "\n",
    "    # Reorder the word_counts_top DataFrame using the sorted_words list\n",
    "    word_counts_top = word_counts_top.loc[sorted_words]\n",
    "\n",
    "    # Then, update the plotting code\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    # Create a heatmap of the word counts\n",
    "    plt.imshow(word_counts_top, cmap='Greens', aspect='auto')\n",
    "    plt.xticks(range(len(word_counts_top.columns)), word_counts_top.columns, rotation=45)\n",
    "    plt.yticks(range(len(word_counts_top.index)), word_counts_top.index)\n",
    "\n",
    "    # Annotate the heatmap with the exact counts of each word in each cell\n",
    "    for i in range(len(word_counts_top.index)):\n",
    "        for j in range(len(word_counts_top.columns)):\n",
    "            count = word_counts_top.iloc[i, j]\n",
    "            plt.text(j, i, str(int(count)), ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "    # Make the category row labels bold\n",
    "    ax = plt.gca()\n",
    "    for category in categories.keys():\n",
    "        if category in word_counts_top.index:\n",
    "            idx = word_counts_top.index.get_loc(category)\n",
    "            ax.get_yticklabels()[idx].set_weight('bold')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.title('Word counts by year: ' + get_company_name(df))\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f32431455b22e24a4c22280dc45085504436fbffe15043106075ea8e5463655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
